# 经典召回模型

召回算法的主要目的是从海量的候选项目（如商品、文章、视频等）中快速筛选出一部分与用户可能感兴趣的项目相关的候选集。

[TOC]

## 基于协同过滤的召回 （Collaborative Filtering）

- 根据用户之前的喜好以及其他兴趣相近的用户的选择来给用户推荐物品。
  - 基于对用户历史行为数据的挖掘发现用户的喜好偏向， 并预测用户可能喜好的产品进行推荐。
  - 一般是仅仅**基于用户的行为数据**（评价、购买、下载等）, 而不依赖于项的任何附加信息（物品自身特征）或者用户的任何附加信息（年龄， 性别等）。

### 相似性度量法

- 给用户推荐和他兴趣相似的其他用户喜欢的产品。

1. **杰卡德（Jaccard）相似系数**：评估用户是否会打分、隐式反馈数据（0-1）
   $$
   sim_{uv}=\frac{|N(u)\cap N(v)|}{|N(u)|\cup|N(v)|}
   $$

   - 其中 `N(u)`，`N(v)` 分别表示用户 `u` 和用户 `v` 交互物品的集合。
   - 对于用户`u` 和 `v` ，该公式反映了两个交互物品交集的数量占这两个用户交互物品并集的数量的比例。

   由于杰卡德相似系数一般无法反映具体用户的评分喜好信息，所以常用来评估用户是否会对某物品进行打分， 而不是预估用户会对某物品打多少分。

2. **余弦相似度**：度量文本相似度、用户相似度、物品相似度
   $$
   sim_{uv}=\frac{|N(u)\cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}
   $$

   ```python
   from sklearn.metrics.pairwise import cosine_similarity
   
   i = [1, 0, 0, 0]
   j = [1, 0, 1, 0]
   cosine_similarity([i, j])
   ```


3. **皮尔逊相关系数**：不适合用作计算布尔值向量（0-1）之间相关度
   $$
   sim(u,v)=\frac{\sum_{i\in I}(r_{ui}-\bar{r}_u)(r_{vi}-\bar{r}_v)}{\sqrt{\sum_{i\in I}(r_{ui}-\bar{r}_u)^2}\sqrt{\sum_{i\in I}(r_{vi}-\bar{r}_v)^2}}
   $$

   - 其中，rui和rvi 分别表示用户 *u* 和用户 *v* 对物品 *i* 是否有交互(或具体评分值)；

   - *r*ˉ*u*,*r*ˉ*v* 分别表示用户 u*u* 和用户 v*v* 交互的所有物品交互数量或者评分的平均值；

   ```python
   from scipy.stats import pearsonr
   
   i = [1, 0, 0, 0]
   j = [1, 0.5, 0.5, 0]
   pearsonr(i, j)
   ```



### 权重优化

- base 公式
  $$
  w_{ij}=\frac{|N(i)\bigcap N(j)|}{|N(i)|}
  $$

  - 该公式表示同时喜好物品 *i* 和物品 j* 的用户数，占喜爱物品 *i* 的比例。
  - 缺点：若物品 *j* 为热门物品，那么它与任何物品的相似度都很高。

- 对热门物品进行惩罚
  $$
  w_{ij}=\frac{|N(i)\cap N(j)|}{\sqrt{|N(i)||N(j)|}}
  $$

  - 根据 base 公式在的问题，对物品 *j* 进行打压。打压的出发点很简单，就是在分母再除以一个物品 *j* 被购买的数量。
  - 此时，若物品 *j* 为热门物品，那么对应的 N(j) 也会很大，受到的惩罚更多。

- 控制对热门物品的惩罚力度
  $$
  w_{ij}=\frac{|N(i)\cap N(j)|}{|N(i)|^{1-\alpha}|N(j)|^\alpha}
  $$

  - 除了第二点提到的办法，在计算物品之间相似度时可以对热门物品进行惩罚外。
  - 可以在此基础上，进一步引入参数 α* ，这样可以通过控制参数 α*来决定对热门物品的惩罚力度。

- 对活跃用户的惩罚
  $$
  w_{ij}=\frac{\sum_{\mathrm{u\in N(i)\cap N(j)}}\frac{1}{\log1+|N(u)|}}{|N(i)|^{1-\alpha}|N(j)|^\alpha}
  $$

  - 在计算物品之间的相似度时，可以进一步将用户的活跃度考虑进来。

  - 对于异常活跃的用户，在计算物品之间的相似度时，他的贡献应该小于非活跃用户。

    

### 基于用户的协同过滤（UserCF）

- **基本思想**

  如果我们要对用户 A 进行物品推荐，可以先找到和他有相似兴趣的其他用户

  然后，将共同兴趣用户喜欢的，但用户 A 未交互过的物品推荐给 A。

  <img src="D:/coding/my_cans/fun-rec/2.BasicAlgorithm/Match/%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B.assets/image-20250218115946124.png" alt="image-20250218115946124" style="zoom: 67%;" />

- **具体过程**

  1. 计算用户之间的相似度

     - Jaccard相似系数、余弦相似度、皮尔逊相关系数

  2. 计算用户对新物品的评分预测

     - 利用目标用户与相似用户之间的相似度以及相似用户对物品的评分，来预测目标用户对候选物品的评分估计
       $$
       R_\mathrm{u,p}=\frac{\sum_{\mathrm{s}\in S}\left(w_\mathrm{u,s}\cdot R_\mathrm{s,p}\right)}{\sum_{\mathrm{s}\in S}w_\mathrm{u,s}}
       $$
       权重 wus 是用户 *u* 和用户 *s* 的相似度， Rs,p 是用户 *s* 对物品 *p* 的评分。

     - 排除用户评分偏置
       $$
       R_{\mathrm{u,p}}=\bar{R}_u+\frac{\sum_{\mathrm{s}\in S}\left(w_{\mathrm{u,s}}\cdot\left(R_{s,p}-\bar{R}_s\right)\right)}{\sum_{\mathrm{s}\in S}w_{\mathrm{u,s}}}
       $$
       其中，Rˉs表示用户 *s* 对物品的历史平均评分

  3. 对用户进行物品推荐

     - 在获得用户 u 对不同物品的评价预测后， 最终的推荐列表根据预测评分进行排序得到。

       

- **算法评价**

  - 召回率：在模型召回预测的物品中，预测准确的物品占用户实际喜欢的物品的比例。
    $$
    \mathrm{Recall}=\frac{\sum_u|R(u)\cap T(u)|}{\sum_u|T(u)|}
    $$

  - 精确率：推荐的物品中，对用户准确推荐的物品占总物品的比例。
    $$
    \mathrm{Precision}=\frac{\sum_u\mid R(u)\cap T(u)|}{\sum_u|R(u)|}
    $$

  - 覆盖率：推荐系统能够推荐出来的物品占总物品集合的比例。

    覆盖率反映了推荐算法发掘长尾的能力， 覆盖率越高， 说明推荐算法越能将长尾中的物品推荐给用户。
    $$
    \mathrm{Coverage~}=\frac{\left|\bigcup_{u\in U}R(u)\right|}{|I|}
    $$

  - 新颖度：用推荐列表中物品的平均流行度，度量推荐结果的新颖度。 

    - 如果推荐出的物品都很热门， 说明推荐的新颖度较低。 由于物品的流行度分布呈长尾分布， 所以为了流行度的平均值更加稳定， 在计算平均流行度时对每个物品的流行度取对数。

      

- **算法缺陷**

  1. 数据稀疏性：用户购买重叠率低

     - 一个大型的电子商务推荐系统一般有非常多的物品，用户可能买的其中不到1%的物品，不同用户之间买的物品重叠性较低，导致算法无法找到一个用户的邻居，即偏好相似的用户。

     - 这导致UserCF不适用于那些正反馈获取较困难的应用场景(如酒店预订， 大件物品购买等低频应用)。

  2. 算法扩展性：维护相似度矩阵，存储空间开销大

     - 需要维护用户相似度矩阵，以便快速的找出 TopN相似用户， 该矩阵的存储开销非常大，存储空间随着用户数量的增加而增加。所以，不适合用户数据量大的情况使用。

       

### 基于物品的协同过滤（ItemCF）

- **基本思想**

  不是计算物品内容属性的相似度，而是主要通过分析用户的行为记录计算物品之间的相似度。

  例如， 物品 A 和物品 C 具有很大的相似度，是因为喜欢物品 A 的用户极可能喜欢物品 C。

  <img src="D:/coding/my_cans/fun-rec/2.BasicAlgorithm/Match/%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B.assets/image-20250218132329804.png" alt="image-20250218132329804" style="zoom: 67%;" />

- **具体过程**

  1. 首先计算一下 物品5 和 Alice购买过的物品1,2,3,4 之间的相似性。
  2. 在Alice购买过的物品中，找出与物品 5 最相近的 n 个物品。
  3. 根据 Alice 对最相近的 n 个物品的打分去计算对物品 5 的打分情况。

- **算法评价**

  - 召回率：在模型召回预测的物品中，预测准确的物品占用户实际喜欢的物品的比例。
    $$
    \mathrm{Recall}=\frac{\sum_u|R(u)\cap T(u)|}{\sum_u|T(u)|}
    $$

  - 精确率：推荐的物品中，对用户准确推荐的物品占总物品的比例。
    $$
    \mathrm{Precision}=\frac{\sum_u\mid R(u)\cap T(u)|}{\sum_u|R(u)|}
    $$

  - 覆盖率：推荐系统能够推荐出来的物品占总物品集合的比例。

    覆盖率反映了推荐算法发掘长尾的能力， 覆盖率越高， 说明推荐算法越能将长尾中的物品推荐给用户。
    $$
    \mathrm{Coverage~}=\frac{\left|\bigcup_{u\in U}R(u)\right|}{|I|}
    $$

  - 新颖度：用推荐列表中物品的平均流行度，度量推荐结果的新颖度。 

    - 如果推荐出的物品都很热门， 说明推荐的新颖度较低。 由于物品的流行度分布呈长尾分布， 所以为了流行度的平均值更加稳定， 在计算平均流行度时对每个物品的流行度取对数。

      

- **算法缺陷**

  1. 泛化能力弱：

     - 即协同过滤无法将两个物品相似的信息推广到其他物品的相似性上。
     - 导致的问题是**热门物品具有很强的头部效应， 容易跟大量物品产生相似， 而尾部物品由于特征向量稀疏， 导致很少被推荐**。

  2. 由于未使用更丰富的用户和物品特征信息，这也导致协同过滤算法的模型表达能力有限。

     

### Swing(Graph-based)

- **基本思想：寻找互补商品**

  之前算法对互补性产品的建模不足，可能会导致用户购买过手机之后还继续推荐手机，但用户短时间内不会再继续购买手机，因此产生无效曝光。所以，Swing 算法利用 user-item 二部图的子结构捕获产品间的替代关系。

  E.g.图中的红色四边形就是一种Swing子结构，这种子结构可以作为给王五推荐尿布的依据。

  <img src="D:/coding/my_cans/fun-rec/2.BasicAlgorithm/Match/%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B.assets/image-20250218134153413.png" alt="image-20250218134153413" style="zoom:67%;" />

- **计算公式**
  $$
  s(i,j)=\sum_{u\in U_i\cap U_j}\sum_{v\in U_i\cap U_j}w_u*w_v*\frac{1}{\alpha+|I_u\cap I_v|}
  $$
  其中Ui是点击过商品i的用户集合，Iu 是用户u点击过的商品集合，α是平滑系数。

  $w_u=\frac{1}{\sqrt{|I_u|}},w_v=\frac{1}{\sqrt{|I_v|}}$是用户权重参数，来降低活跃用户的影响。

- **改进：Surprise算法**

  - 类别层面
    
    - 由于类别直接的种类差异，每个类别的相关类数量存在差异，因此采用最大相对落点来作为划分阈值。
    
    - 例如图(a)中T恤的相关类选择前八个，图(b)中手机的相关类选择前三个。
    
      <img src="D:/coding/my_cans/fun-rec/2.BasicAlgorithm/Match/%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B.assets/image-20250218141121625.png" alt="image-20250218141121625" style="zoom:67%;" />
    
  - 商品层面

    - 考虑商品的购买顺序，例如在用户购买手机后推荐充电宝是合理的，但在用户购买充电宝后推荐手机是不合理的。

    - 考虑两个商品购买的时间间隔，时间间隔越短越能证明两个商品的互补关系。
  
  - 聚类层面
  
    - 如何聚类？ 传统的聚类算法（基于密度和 k-means ）在数十亿产品规模下的淘宝场景中不可行，所以作者采用了标签传播算法。
    - 在哪里标签传播？ Item-item 图，其中有 Swing 计算的排名靠前 item 为邻居，边的权重就是 Swing 分数。
    - 表现如何？ 快速而有效，15分钟即可对数十亿个项目进行聚类。 最终聚类层面的相关度计算同上面商品层面的计算公式
  

### 矩阵分解MF（隐语义模型LFM）

- **基本思想**

  隐语义模型，就是先通过一些角度把用户兴趣和这些书归一下类， 当来了用户之后， 首先得到他的兴趣分类， 然后从这个分类中挑选他可能喜欢的书籍。

  <img src="D:/coding/my_cans/fun-rec/2.BasicAlgorithm/Match/%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B.assets/image-20250218142538819.png" alt="image-20250218142538819" style="zoom:67%;" />

- **计算过程**

  1. 基于评分矩阵，将其分解成Q和P两个矩阵乘积的形式，获取用户兴趣和物品的隐向量表达。

     - 一般而言， k越大，隐向量能承载的信息内容越多，表达能力也会更强，但相应的学习难度也会增加。
     - 所以，我们需要根据训练集样本的数量去选择合适的数值，在保证信息学习相对完整的前提下，降低模型的学习难度，根据经验， 随机数需要和`1/sqrt(F)`成正比。

     <img src="D:/coding/my_cans/fun-rec/2.BasicAlgorithm/Match/%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B.assets/image-20250218142416958.png" alt="image-20250218142416958"  />

  2. 然后，基于两个分解矩阵去预测某个用户对某个物品的评分了

  3. 最后，基于预测评分去进行物品推荐。

  $$
  \operatorname{Preference}(u, i)=r_{u i}=p_{u}^{T} q_{i}=\sum_{k=1}^{K} p_{u, k} q_{i, k}
  $$

  ​	

- **具体算法实现**

  - FunkSVD（LFM）

    把求解上面两个矩阵的参数问题转换成一个最优化问题， 可以通过训练集里面的观察值利用最小化来学习用户矩阵和物品矩阵。

    1. 根据前面提到的，在有用户矩阵和物品矩阵的前提下，若要计算用户 $u$ 对物品 $i$ 的评分， 可以根据公式：
       $$
       \operatorname{Preference}(u, i)=r_{u i}=p_{u}^{T} q_{i}=\sum_{k=1}^{K} p_{u, k} q_{i,k}
       $$

       + 其中，向量 $p_u$ 表示用户 $u$ 的隐向量，向量  $q_i$ 表示物品 $i$ 的隐向量。

    2. 随机初始化一个用户矩阵 $U$ 和一个物品矩阵 $V$，获取每个用户和物品的初始隐语义向量。

    3. 将用户和物品的向量内积 $p_{u}^{T} q_{i}$， 作为用户对物品的预测评分 $\hat{r}_{u i}$。

       +  $\hat{r}_{u i}=p_{u}^{T} q_{i}$ 表示的是通过建模，求得的用户 $u$ 对物品的预测评分。
       + 在用户对物品的评分矩阵中，矩阵中的元素 $r_{u i}$ 才是用户对物品的真实评分。

    4. 对于评分矩阵中的每个元素，计算预测误差 $e_{u i}=r_{u i}-\hat{r}_{u i}$，对所有训练样本的平方误差进行累加：
       $$
       \operatorname{SSE}=\sum_{u, i} e_{u i}^{2}=\sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{i,k}\right)^{2}
       $$

       + 从上述公式可以看出，$SSE$ 建立起了训练数据和预测模型之间的关系。
         
       + 如果我们希望模型预测的越准确，那么在训练集（已有的评分矩阵）上的预测误差应该仅可能小。
         
       + 为方便后续求解，给 $SSE$ 增加系数 $1/2$ ：
         $$
         \operatorname{SSE}=\frac{1}{2} \sum_{u, i} e_{u i}^{2}=\frac{1}{2} \sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u k} q_{i k}\right)^{2}
         $$

    5.  前面提到，模型预测越准确等价于预测误差越小，那么优化的目标函数变为：
       $$
       \min _{\boldsymbol{q}^{*}, \boldsymbol{p}^{*}} \frac{1}{2} \sum_{(u, i) \in K}\left(\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}\right)^{2}
       $$

       + $K$ 表示所有用户评分样本的集合，**即评分矩阵中不为空的元素**，其他空缺值在测试时是要预测的。
       + 该目标函数需要优化的目标是用户矩阵 $U$ 和一个物品矩阵 $V$。

    6. 对于给定的目标函数，可以通过梯度下降法对参数进行优化。

       + 求解目标函数 $SSE$ 关于用户矩阵中参数 $p_{u,k}$ 的梯度：
         $$
         \frac{\partial}{\partial p_{u,k}} S S E=\frac{\partial}{\partial p_{u,k}}\left(\frac{1}{2}e_{u i}^{2}\right) =e_{u i} \frac{\partial}{\partial p_{u,k}} e_{u i}=e_{u i} \frac{\partial}{\partial p_{u,k}}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{i,k}\right)=-e_{u i} q_{i,k}
         $$

       + 求解目标函数 $SSE$ 关于 $q_{i,k}$ 的梯度：
         $$
         \frac{\partial}{\partial q_{i,k}} S S E=\frac{\partial}{\partial q_{i,k}}\left(\frac{1}{2}e_{u i}^{2}\right) =e_{u i} \frac{\partial}{\partial q_{i,k}} e_{u i}=e_{u i} \frac{\partial}{\partial q_{i,k}}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{i,k}\right)=-e_{u i} p_{u,k}
         $$

    7. 参数梯度更新
       $$
       p_{u, k}=p_{u,k}-\eta (-e_{ui}q_{i, k})=p_{u,k}+\eta e_{ui}q_{i, k} \\ 
       q_{i, k}=q_{i,k}-\eta (-e_{ui}p_{u,k})=q_{i, k}+\eta e_{ui}p_{u, k}
       $$

       + 其中，$\eta$ 表示学习率， 用于控制步长。
       
       + 但上面这个有个问题就是当参数很多的时候， 就是两个矩阵很大的时候， 往往容易陷入过拟合的困境， 这时候， 就需要在目标函数上面加上正则化的损失， 就变成了RSVD
       
         

    **优化：RSVD（加入正则项）**

    为了控制模型的复杂度。在原有模型的基础上，加入 $l2$ 正则项，来防止过拟合。

    + 当模型参数过大，而输入数据发生变化时，可能会造成输出的不稳定。

    + $l2$ 正则项等价于假设模型参数符合0均值的正态分布，从而使得模型的输出更加稳定。

    $$
    \min _{\boldsymbol{q}^{*}, \boldsymbol{p}^{*}} \frac{1}{2} \sum_{(u, i) \in K}\left(\boldsymbol{r}_{\mathrm{ui}}-p_{u}^{T} q_{i}\right)^{2} + \lambda\left(\left\|p_{u}\right\|^{2}+\left\|q_{i}\right\|^{2}\right)
    $$

    

  - BiasSVD

    提出了另一种LFM， 在原来的基础上加了偏置项， 来消除用户和物品打分的偏差。

    $$
    \hat{r}_{u i}=\mu+b_{u}+b_{i}+p_{u}^{T} \cdot q_{i}
    $$
    1. **预测公式**

       - 这个预测公式加入了3项偏置参数 $\mu,b_u,b_i$, 作用如下：

       - $\mu$： 该参数反映的是推荐模型整体的平均评分，一般使用所有样本评分的均值。

       - $b_u$：用户偏差系数。可以使用用户 $u$ 给出的所有评分的均值， 也可以当做训练参数。 
         - 这一项表示了用户的评分习惯中和物品没有关系的那种因素。 比如有些用户比较苛刻， 对什么东西要求很高， 那么他评分就会偏低， 而有些用户比较宽容， 对什么东西都觉得不错， 那么评分就偏高

       - $b_i$：物品偏差系数。可以使用物品 $i$ 收到的所有评分的均值， 也可以当做训练参数。 
         - 这一项表示了物品接受的评分中和用户没有关系的因素。 比如有些物品本身质量就很高， 因此获得的评分相对比较高， 有的物品本身质量很差， 因此获得的评分相对较低。

    2. **目标函数**

       在加入正则项的FunkSVD的基础上，BiasSVD 的目标函数如下：
       $$
       \begin{aligned}
       \min _{q^{*}, p^{*}} \frac{1}{2} \sum_{(u, i) \in K} &\left(r_{u i}-\left(\mu+b_{u}+b_{i}+q_{i}^{T} p_{u}\right)\right)^{2} \\
       &+\lambda\left(\left\|p_{u}\right\|^{2}+\left\|q_{i}\right\|^{2}+b_{u}^{2}+b_{i}^{2}\right)
       \end{aligned}
       $$
       可得偏置项的梯度更新公式如下：

       + $\frac{\partial}{\partial b_{i}} S S E=-e_{u i}+\lambda b_{i}$
       + $\frac{\partial}{\partial b_{u}} S S E=-e_{u i}+\lambda b_{u}$

         

- **算法评价**

  - 优点：
    - 泛化能力强： 一定程度上解决了稀疏问题
    - 空间复杂度低： 由于用户和物品都用隐向量的形式存放， 少了用户和物品相似度矩阵， 空间复杂度由$n^2$降到了$(n+m)*f$
    - 更好的扩展性和灵活性：矩阵分解的最终产物是用户和物品隐向量， 这个深度学习的embedding思想不谋而合， 因此矩阵分解的结果非常便于与其他特征进行组合和拼接， 并可以与深度学习无缝结合。

  - 缺点：

    - 矩阵分解算法依然是只用到了评分矩阵， 没有考虑到用户特征， 物品特征和上下文特征， 这使得矩阵分解丧失了利用很多有效信息的机会。

    - 同时在缺乏用户历史行为的时候， 无法进行有效的推荐。

    - 为了解决这个问题， **逻辑回归模型及后续的因子分解机模型**， 凭借其天然的融合不同特征的能力， 逐渐在推荐系统领域得到了更广泛的应用。

      

## 基于向量的召回

### FM召回

- **基本思想**

  FM会把每个特征（比如用户的年龄、性别，商品的类别、价格）转化为一个**向量**（比如长度为10的数字数组）。

  这些向量能捕捉特征之间的隐含关系，比如“年轻人”和“运动鞋”这两个特征的向量可能比较接近。

- **算法原理**

  1. **原始FM公式分为两部分：**
     $$
     \hat{y}(\mathbf{x}):=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle x_{i} x_{j}
     $$

     - **一阶特征**：直接判断每个特征的重要性（比如“价格低”本身就很吸引人）。

       在 FM 的表达式中，前两项为特征的一阶交互项。将其拆分为用户特征和物品特征的一阶特征交互项，如下：
       $$
       \begin{aligned}
       & w_{0}+\sum_{i=1}^{n} w_{i} x_{i} \\
       &= w_{0} + \sum_{t \in I}w_{t} x_{t} + \sum_{u\in U}w_{u} x_{u} \\
       \end{aligned}
       $$

       + 其中，$U$ 表示用户相关特征集合，$I$ 表示物品相关特征集合。

     - **二阶特征交互**：判断特征组合的重要性（比如“年轻人 + 运动鞋”组合更匹配）。

       观察 FM 的二阶特征交互项，可知其计算复杂度为 $O\left(k n^{2}\right)$ 。为了降低计算复杂度，按照如下公式进行变换。
       $$
       \begin{aligned}
       & \sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle x_{i} x_{j} \\
       =& \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{j}\right\rangle x_{i} x_{j}-\frac{1}{2} \sum_{i=1}^{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{i}\right\rangle x_{i} x_{i} \\
       =& \frac{1}{2}\left(\sum_{i=1}^{n} \sum_{j=1}^{n} \sum_{f=1}^{k} v_{i, f} v_{j, f} x_{i} x_{j}-\sum_{i=1}^{n} \sum_{f=1}^{k} v_{i, f} v_{i, f} x_{i} x_{i}\right) \\
       =& \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{}\left(\sum_{j=1}^{n} v_{j, f} x_{j}\right)-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right) \\
       =& \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right)
       \end{aligned}
       $$
       + 公式变换后，计算复杂度由 $O\left(k n^{2}\right)$ 降到 $O\left(k n\right)$。

       由于本文章需要将 FM 模型用在召回，故将二阶特征交互项拆分为用户和物品项。有：
       $$
       \begin{aligned}
       & \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right) \\
       =& \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{u \in U} v_{u, f} x_{u} + \sum_{t \in I} v_{t, f} x_{t}\right)^{2}-\sum_{u \in U} v_{u, f}^{2} x_{u}^{2} - \sum_{t\in I} v_{t, f}^{2} x_{t}^{2}\right) \\
       =& \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{u \in U} v_{u, f} x_{u}\right)^{2} + \left(\sum_{t \in I} v_{t, f} x_{t}\right)^{2} + 2{\sum_{u \in U} v_{u, f} x_{u}}{\sum_{t \in I} v_{t, f} x_{t}} - \sum_{u \in U} v_{u, f}^{2} x_{u}^{2} - \sum_{t \in I} v_{t, f}^{2} x_{t}^{2}\right)  
       \end{aligned}
       $$

  2. **在召回阶段，我们需要高效计算，所以FM被巧妙拆解为两个向量（用户向量+商品向量）**

     合并 FM 的一阶、二阶特征交互项，得到基于 FM 召回的匹配分计算公式：
     $$
     \text{MatchScore}_{FM} = \sum_{t \in I} w_{t} x_{t} + \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{t \in I} v_{t, f} x_{t}\right)^{2}  - \sum_{t \in I} v_{t, f}^{2} x_{t}^{2}\right)  + \sum_{f=1}^{k}\left( {\sum_{u \in U} v_{u, f} x_{u}}{\sum_{t \in I} v_{t, f} x_{t}} \right)
     $$
     - 用户向量：
       $$
       V_{user} = [1; \quad {\sum_{u \in U} v_{u} x_{u}}]
       $$

       + 用户向量由两项表达式拼接得到。
       + 第一项为常数 $1$，第二项是将用户相关的特征向量进行 sum pooling 。

     + 物品向量：
       $$
       V_{item} = [\sum_{t \in I} w_{t} x_{t} + \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{t \in I} v_{t, f} x_{t}\right)^{2}  - \sum_{t \in I} v_{t, f}^{2} x_{t}^{2}\right); \quad
       {\sum_{t \in I} v_{t} x_{t}} ]
       $$

       + 第一项表示物品相关特征向量的一阶、二阶特征交互。
       + 第二项是将物品相关的特征向量进行 sum pooling 。

     + 最终匹配分 = 用户向量 · 商品向量
       $$
       \text{MatchScore}_{FM} = V_{item} V_{user}^T
       $$

       

### item2vec系列

#### word2vec

- **基本思想**

  1. 单词的上下文很重要

     一个单词的含义可以通过它周围的单词来理解。比如，句子“我每天都会喝一杯咖啡”中，“咖啡”周围的单词是“喝”、“一杯”等。通过分析这些上下文单词，我们可以知道“咖啡”是一种可以喝的东西。

  2. **把单词变成向量**

     word2vec 把每个单词表示成一个向量（一组数字）。这些向量的维度可以自己设定，比如 100 维或 300 维。这些向量的神奇之处在于，语义相近的单词，它们的向量在数学上也会很接近。

- **算法原理**（两个模型：Skip-gram和CBOW）

  - **Skip-gram**（给定一个中心词，预测它周围的单词）

  - **CBOW**（给定周围的单词，预测中心词）

    下面以Skip-gram为例说明

    <img src="D:/coding/my_cans/fun-rec/2.BasicAlgorithm/Match/%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B.assets/image-20250219161931554.png" alt="image-20250219161931554" style="zoom:67%;" />

    那么，如果我们在整个语料库上不断地滑动窗口，我们可以得到所有位置的$P(o|c)$，我们希望在所有位置上**最大化单词o在单词c周围出现了这一事实**。由极大似然法，可得损失函数为：$max\prod_{c} \prod_{o}P(o|c)$

    1. P(o|c)是一个概率，所以我们在整个语料库上使用**softmax**将点积的值映射到概率，
       $$
       P(o|c)=softmax(u_{o^T} \cdot v_c)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{\substack{m \leq j \leq m \\ j \neq 0}} \log P\left(w_{t+j} \mid w_{t} ; \theta\right)
       $$

    2. 中心词词向量为$v_{c}$,而上下文词词向量为$u_{o}$。损失函数是关于$u_{o}$和$v_c$的函数，我们通过梯度下降法调整$u_{o}$和$v_c$的值，最小化损失函数，即得到了良好表示的词向量。

  - **负采样**

    在训练过程中，计算所有单词的概率是非常耗时的。word2vec 使用了一种叫 **负采样** 的技巧来加速训练。

    负采样的核心思想是：除了真实的上下文单词（比如“咖啡”和“喝”），我们还会随机选择一些不相关的单词（比如“飞机”、“电脑”等），并告诉模型这些单词不是上下文。通过这种方式，模型可以更快地学习哪些单词是相关的，哪些不是。

  


#### item2vec

- **基本思想**

  Item2Vec 的原理十分十分简单，它是基于 Skip-Gram 模型的物品向量训练方法。但又存在一些区别，如下：

  + 词向量的训练是基于句子序列（sequence），但是物品向量的训练是基于物品集合（set）。
  + 因此，物品向量的训练丢弃了空间、时间信息。

- **算法原理**

  Item2Vec 论文假设对于一个集合的物品，它们之间是相似的，与用户购买它们的顺序、时间无关。当然，该假设在其他场景下不一定使用，但是原论文只讨论了该场景下它们实验的有效性。由于忽略了空间信息，原文将共享同一集合的每对物品视为正样本。目标函数如下：
  $$
  \frac{1}{K} \sum_{i=1}^{K} \sum_{j \neq i}^{K} \log p\left(w_{j} \mid w_{i}\right)
  $$

  + 对于窗口大小 $K$，由设置决定。

  在 Skip-Gram 模型中，提到过每个单词 $w_i$ 有2个特征表示。在 Item2Vec 中同样如此，论文中是将物品的中心词向量 $u_i$ 作为物品的特征向量。作者还提到了其他两种方式来表示物品向量：

  + **add**：$u_i + v_i$
  + **concat**：$\left[u_{i}^{T} v_{i}^{T}\right]^{T}$

  

#### Airbnb案例

原论文链接：https://dl.acm.org/doi/pdf/10.1145/3219819.3219885

- **基本思想**

  基于向量的召回是一种利用 **Embedding 技术** 来优化推荐和搜索的方法。简单来说，就是把用户和房源都表示成高维空间中的向量，通过计算向量之间的相似度来找到用户可能感兴趣的房源。

  这种方法的核心在于：①如何生成这些向量（Embedding），以及②如何利用这些向量进行有效的推荐。

- **如何生成向量**

  - **用于描述短期实时性的个性化特征 Embedding：****listing Embeddings**

    Listing Embeddings 是基于用户的点击 session 学习得到的，用于表示房源的短期实时性特征。给定数据集 $ \mathcal{S} $ ，其中包含了 $ N $ 个用户的 $ S $ 个点击 session（序列）。

    - 每个 session $ s=\left(l_{1}, \ldots, l_{M}\right) \in \mathcal{S} $ ，包含了 $ M $ 个被用户点击过的 listing ids 。

    - 对于用户连续两次点击，若时间间隔超过了30分钟，则启动新的 session。

    1. 优化：构建正负样本集

       - 将booked listing 作为全局正样本。

         booked listing 表示用户在 session 中最终预定的房源，一般只会出现在结束的 session 中。Airbnb 将最终预定的房源，始终作为滑窗的上下文，即全局上下文。如图所示，对于当前滑动窗口的 central listing，实线箭头表示context listings，虚线（指向booked listing）表示 global context listing。

         <img src="D:/coding/my_cans/fun-rec/2.BasicAlgorithm/Match/%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B.assets/image-20250219174319624.png" alt="image-20250219174319624" style="zoom:67%;" />

         - 优化的目标函数更新为：
           $$
           \underset{\theta}{\operatorname{argmax}} \sum_{(l, c) \in \mathcal{D}_{p}} \log \frac{1}{1+e^{-\mathbf{v}_{c}^{\prime^{\prime}} \mathbf{v}_{l}}}+\sum_{(l, c) \in \mathcal{D}_{n}} \log \frac{1}{1+e^{\mathbf{v}_{c}^{\prime} \mathbf{v}_{l}}} +
           \log \frac{1}{1+e^{-\mathbf{v}_{c}^{\prime} \mathbf{v}_{l_b}}}
           $$


       - 优化负样本的选择
          - 用户通过在线网站预定房间时，通常只会在同一个 market （将要停留区域）内进行搜索。
          
          - 对于用户点击过的样本集 $ \mathcal{D}_{p} $ （正样本集）而言，它们大概率位于同一片区域。考虑到负样本集 $ \mathcal{D}_{n} $ 是随机抽取的，大概率来源不同的区域。
          
          - 解决办法也很简单，对于每个滑窗中的中心 lisitng，其负样本的选择新增了与其位于同一个 market 的 listing。至此，优化函数更新如下：
            $$
            \underset{\theta}{\operatorname{argmax}} \sum_{(l, c) \in \mathcal{D}_{p}} \log \frac{1}{1+e^{-\mathbf{v}_{c}^{\prime^{\prime}} \mathbf{v}_{l}}}+\sum_{(l, c) \in \mathcal{D}_{n}} \log \frac{1}{1+e^{\mathbf{v}_{c}^{\prime} \mathbf{v}_{l}}} +\log \frac{1}{1+e^{-\mathbf{v}_{c}^{\prime} \mathbf{v}_{l_b}}} + 
            \sum_{(l, m_n ) \in \mathcal{D}_{m_n}} \log \frac{1}{1+e^{\mathbf{v}_{m_n}^{\prime} \mathbf{v}_{l}}}
            $$
          
            +  $ \mathcal{D}_{m_n} $ 表示与滑窗中的中心 listing 位于同一区域的负样本集。

    2. 优化：Listing Embedding 的冷启动

       - Airbnb 每天都有新的 listings 产生，而这些 listings 却没有 Embedding 向量表征。
       - Airbnb 建议利用其他 listing 的现有的 Embedding 来为新的 listing 创建 Embedding。
         - 在新的 listing 被创建后，房主需要提供如位置、价格、类型等在内的信息。
         - 然后利用房主提供的房源信息，为其查找3个相似的 listing，并将它们 Embedding 的均值作为新 listing 的 Embedding表示。
         - 这里的相似，包含了位置最近（10英里半径内），房源类型相似，价格区间相近。
       - 通过该手段，Airbnb 可以解决 98% 以上的新 listing 的 Embedding 冷启动问题。

    3. 最大化目标

       在拿到多个用户点击的 session 后，可以基于 Word2Vec 的 Skip-Gram 模型来学习不同 listing 的 Embedding 表示。最大化目标函数 $ \mathcal{L} $​ ：
       $$
       \mathcal{L}=\sum_{s \in \mathcal{S}} \sum_{l_{i} \in s}\left(\sum_{-m \geq j \leq m, i \neq 0} \log \mathbb{P}\left(l_{i+j} \mid l_{i}\right)\right)
       $$
       概率 $ \mathbb{P}\left(l_{i+j} \mid l_{i}\right) $ 是基于 soft-max 函数的表达式。表示在一个 session 中，已知中心 listing $ l_i $ 来预测上下文 listing $ l_{i+j} $​​ 的概率：
       $$
       \mathbb{P}\left(l_{i+j} \mid l_{i}\right)=\frac{\exp \left(\mathbf{v}_{l_{i}}^{\top} \mathbf{v}_{l_{i+j}}^{\prime}\right)}{\sum_{l=1}^{|\mathcal{V}|} \exp \left(\mathbf{v}_{l_{i}}^{\top} \mathbf{v}_{l}^{\prime}\right)}
       $$

       - 其中， $ \mathbf{v}_{l_{i}} $ 表示 listing $ l_i $ 的 Embedding 向量， $ |\mathcal{V}| $ 表示全部的物料库的数量。

    4. 负采样优化

       考虑到物料库 $ \mathcal{V} $ 过大，模型中参数更新的时间成本和 $ |\mathcal{V}| $ 成正比。为了降低计算复杂度，要进行负采样。负采样后，优化的目标函数如下：
       $$
       \underset{\theta}{\operatorname{argmax}} \sum_{(l, c) \in \mathcal{D}_{p}} \log \frac{1}{1+e^{-\mathbf{v}_{c}^{\prime^{\prime}} \mathbf{v}_{l}}}+\sum_{(l, c) \in \mathcal{D}_{n}} \log \frac{1}{1+e^{\mathbf{v}_{c}^{\prime} \mathbf{v}_{l}}}
       $$

  - **用于描述长期的个性化特征 Embedding：user-type & listing type Embeddings**

    - 面临的挑战：

      - booking sessions $ \mathcal{S}_{b} $ 数据量的大小远远小于 click sessions $ \mathcal{S} $ ，因为预定本身就是一件低频率事件。
      - 许多用户过去只预定了单个数量的房源，无法从长度为1的 session 中学习 Embedding
      - 对于任何实体，要基于 context 学习到有意义的 Embedding，该实体至少在数据中出现5-10次。
         - 但平台上大多数 listing_ids 被预定的次数低于5-10次。
      - 用户连续两次预定的时间间隔可能较长，在此期间用户的行为（如价格敏感点）偏好可能会发生改变（由于职业的变化）。

    - 解决

      将用户的预订历史（booking session）映射到 User-type 和 Listing-type 的序列中，Airbnb 再次利用 Skip-Gram 模型学习这些类型标签的 Embedding。

      - 例子：小王，是一名西藏人，性别男，今年21岁，就读于中国山东的蓝翔技校的挖掘机专业。

        通常，对于不同的用户（如小王），给定一个 ID 编码，然后学习相应的 User Embedding。

        - 但前面说了，用户数据过于稀疏，学习到的 User Embedding 特征表达能力不好。

        另一种方式：利用小王身上的用户标签，先组合出他的 User-type，然后学习 Embedding 表示。
        - 小王的 User-type：西藏人_男_学生_21岁_位置中国山东_南翔技校_挖掘机专业。
        - 组合得到的 User-type 本质上可视为一个 Category 特征，然后学习其对应的 Embedding 表示。

    - 优化：

      - 联合训练 User-type Embedding 和 Listing-type Embedding

        为了学习在相同向量空间中的 User-type 和 Listing-type 的 Embeddings，Airbnb 的做法是将 User-type 插入到 booking sessions 中。形成一个（User-type, Listing-type）组成的元组序列，这样就可以让 User-type 和 Listing-type 的在 session 中的相对位置保持一致了。

        - User-type 的目标函数：
             $$
             \underset{\theta}{\operatorname{argmax}} \sum_{\left(u_{t}, c\right) \in \mathcal{D}_{b o o k}} \log \frac{1}{1+e^{-\mathbf{v}_{c}^{\prime} \mathbf{v}_{u_{t}}}}+\sum_{\left(u_{t}, c\right) \in \mathcal{D}_{n e g}} \log \frac{1}{1+e^{\mathbf{v}_{c}^{\prime} \mathbf{v}_{u_{t}}}}
             $$

             +  $ \mathcal{D}_{\text {book }} $ 中的 $ u_t $ （中心词）表示 User-type， $ c $ （上下文）表示用户最近的预定过的 Listing-type。 $ \mathcal{D}_{\text {neg}} $ 中的 $ c $ 表示 negative Listing-type。

             +  $ u_t $ 表示 User-type 的 Embedding， $ \mathbf{v}_{c}^{\prime} $ 表示 Listing-type 的Embedding。

        - Listing-type 的目标函数：
             $$
             \begin{aligned}
             \underset{\theta}{\operatorname{argmax}} & \sum_{\left(l_{t}, c\right) \in \mathcal{D}_{b o o k}} \log \frac{1}{1+\exp ^{-\mathrm{v}_{c}^{\prime} \mathbf{v}_{l_{t}}}}+\sum_{\left(l_{t}, c\right) \in \mathcal{D}_{n e g}} \log \frac{1}{1+\exp ^{\mathrm{v}_{c}^{\prime} \mathbf{v}_{l_{t}}}} \\
             \end{aligned}
             $$

             - 同理，不过窗口中的中心词为 Listing-type， 上下文为 User-type。

      - Explicit Negatives for Rejections

         - 用户预定房源以后，还要等待房源主人的确认，主人可能接受或者拒绝客人的预定。拒接的原因可能包括，客人星级评定不佳，资料不完整等。

         - 前面学习到的 User-type Embedding 包含了客人的兴趣偏好，Listing-type Embedding 包含了房源的属性特征。

           - 但是，用户的 Embedding 未包含更容易被哪类房源主人拒绝的潜语义信息。
           - 房源的 Embedding 未包含主人对哪类客人的拒绝偏好。

         - 为了提高用户预定房源以后，被主人接受的概率。同时，降低房源主人拒绝客人的概率。Airbnb 在训练 User-type 和 Listing-type 的 Embedding时，将用户预定后却被拒绝的样本加入负样本集中（如下图b）。

           - 更新后，Listing-type 的目标函数：


           $$
           \begin{aligned}
           \underset{\theta}{\operatorname{argmax}} & \sum_{\left(u_{t}, c\right) \in \mathcal{D}_{b o o k}} \log \frac{1}{1+\exp ^{-\mathbf{v}_{c}^{\prime} \mathbf{v}_{u_{t}}}}+\sum_{\left(u_{t}, c\right) \in \mathcal{D}_{n e g}} \log \frac{1}{1+\exp ^{\mathbf{v}_{c}^{\prime} \mathbf{v}_{u_{t}}}} \\
           &+\sum_{\left(u_{t}, l_{t}\right) \in \mathcal{D}_{\text {reject }}} \log \frac{1}{1+\exp ^{\mathrm{v}_{{l_{t}}}^{\prime} \mathrm{v}_{u_{t}}}} 
           \end{aligned}
           $$

           - 更新后，User-type 的目标函数：

           $$
           \begin{aligned}
           \underset{\theta}{\operatorname{argmax}} & \sum_{\left(l_{t}, c\right) \in \mathcal{D}_{b o o k}} \log \frac{1}{1+\exp ^{-\mathrm{v}_{c}^{\prime} \mathbf{v}_{l_{t}}}}+\sum_{\left(l_{t}, c\right) \in \mathcal{D}_{n e g}} \log \frac{1}{1+\exp ^{\mathrm{v}_{c}^{\prime} \mathbf{v}_{l_{t}}}} \\
           &+\sum_{\left(l_{t}, u_{t}\right) \in \mathcal{D}_{\text {reject }}} \log \frac{1}{1+\exp ^{\mathrm{v}^{\prime}_{u_{t}} \mathrm{v}_{l_{t}}}}
           \end{aligned}
           $$

  - **如何利用这些向量进行有效的推荐**

    1. 相似房源推荐

       当用户查看某个房源的详情页时，Airbnb 会利用 Listing Embedding 来找到与当前房源相似的其他房源。具体方法是：

       - 计算当前房源的 Embedding 与其他房源 Embedding 的余弦相似度。

       - 选择相似度最高的几个房源作为“相似房源”推荐给用户。

    2. 实时个性化搜索排序

       在用户进行搜索时，Airbnb 不仅会根据传统的搜索信号（如关键词匹配、价格等）来排序，还会利用 Embedding 来实现 实时个性化。具体步骤如下：

       1. Query Embedding：Airbnb 会对用户的搜索关键词（Query）也进行 Embedding，这样可以捕捉用户模糊查询的真实意图。比如，用户搜索“巴黎海滩别墅”，Query Embedding 可以帮助系统理解用户的真实需求，而不仅仅是关键词匹配。
       2. 特征构建：结合用户的短期兴趣（通过 Listing Embedding）和长期兴趣（通过 User-type & Listing-type Embedding），计算用户与每个搜索结果房源之间的相似度。
       3. 排序模型：将这些相似度特征输入到一个排序模型（如 GBDT）中，最终根据模型的预测分数对搜索结果进行排序。



### YoutubeDNN

- **挑战**

  1. Scale(规模)：视频数量非常庞大，大规模数据下需要分布式学习算法以及高效的线上服务系统

     - 召回模型线下训练的时候，采用了负采样的思路。
     - 线上服务的时候，采用了hash映射，然后近邻检索的方式来满足实时性的需求。

  2. Freshness(新鲜度)：YouTube上的视频是一个动态的， 用户实时上传，且实时访问，那么这时候， 最新的视频往往就容易博得用户的眼球， 用户一般都比较喜欢看比较新的视频， 而不管是不是真和用户相关， 这时候，就需要模型有建模新上传内容以及用户最新发生的行为能力。 为了让模型学习到用户对新视频有偏好， 后面策略里面加了一个"example age"作为体现。

  3. Noise(噪声): 由于数据的稀疏和不可见的其他原因， 数据里面的噪声非常之多，这时候，就需要让这个推荐系统变得鲁棒。

     鲁棒性涉及到召回和排序两块，召回上需要考虑更多实际因素，比如非对称消费特性，高活用户因素，时间因素，序列因素等，并采取了相应的措施， 而排序上做更加细致的特征工程， 尽量的刻画出用户兴趣以及视频的特征优化训练目标，使用加权的逻辑回归等。而召回和排序模型上，都采用了深度神经网络，通过特征的相互交叉，有了更强大的建模能力， 相比于之前用的MF(矩阵分解)， 建模能力上有了很大的提升， 这些都有助于帮助减少噪声， 使得推荐结果更加准确。

- **算法原理**

  <img src="D:/coding/my_cans/fun-rec/2.BasicAlgorithm/Match/%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B.assets/image-20250219201935938.png" alt="image-20250219201935938" style="zoom:67%;" />

  1. **输入（用户侧特征）**

     - 用户历史序列

       包括用户的观看历史和历史搜索等序列性特征，一般是高维稀疏的ID列表：`[item_id5, item_id2, item_id3, ...]`

       ①首先通过 Embedding 层转换为低维稠密向量。例如，每个视频 ID 对应一个 256 维的 Embedding 向量

       ②然后通过average pooling（每一维求平均）得到一个最终向量来表示用户的历史兴趣或搜索兴趣。

       > 论文里面使用了用户最近的50次观看历史，用户最近50次搜索历史token， embedding维度是256维， 采用的average pooling。  当然，这里还可以把item的类别信息也映射到embedding， 与前面的concat起来。

     - 用户的人文特征

       包括用户的地理位置、性别、年龄等。这些特征经过编码和归一化处理后输入模型。

       离散型特征通过 Label Encoder 转换为数值后进行 Embedding，连续型特征则先进行归一化处理。此外，还会加入一些非线性特征（如$x^2$，$logx$）以增强模型的表达能力。

     - example age

       表示用户行为的时间新鲜度，帮助模型捕捉用户的短期兴趣变化。

       "example age"定义为$t_{max}-t$， 其中$t_{max}$是训练数据中所有样本的时间最大值， 而$t$为当前样本的时间。

       **线上预测时， 直接把example age全部设为0或一个小的负值，这样就不依赖于各个视频的上传时间了**。 

       > 我理解是这样，假设没有这样一个example age特征表示视频新颖信息，或者一个位置特征表示商品的位置信息，那模型训练的样本，可能是用户点击了这个item，就是正样本， 但此时有可能是用户真的喜欢这个item， 也有可能是因为一些bias， 比如用户本身喜欢新颖， 用户本身喜欢点击上面位置的item等， 但模型推理的时候，都会误认为是用户真的喜欢这个item。 所以，为了让模型了解到可能是存在后面这种bias， 我们就把item的新颖信息， item的位置信息等做成特征， 在模型训练的时候就告诉模型，用户点了这个东西可能是它比较新或者位置比较靠上面等，这样模型在训练的时候， 就了解到了这些bias，等到模型在线推理的时候呢， 我们把这些bias特征都弄成一样的，这样每个样品在模型看来，就没有了新颖信息和位置信息bias(一视同仁了)，只能靠着相关性去推理， 这样才能推到用户真正感兴趣的东西吧。

  2. **DNN层**

     将上述所有特征拼接成一个长向量，然后通过一个三层的 DNN 进行特征交叉和降维。DNN 的输出是一个低维的用户向量，表示用户当前的兴趣状态。

  3. **输出层**

     基于 **Word2Vec 的 Skip-Gram 模型** 的思想设计的，用于计算用户向量和视频向量之间的相似度，从而实现高效的视频召回。

     <img src="D:/coding/my_cans/fun-rec/2.BasicAlgorithm/Match/%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B.assets/image-20250219204333062.png" alt="image-20250219204333062" style="zoom:67%;" />

     - 向量生成

       - 用户向量：其实就是全连接的DNN网络的输出向量

       - 视频向量：

         ①通过 Word2Vec 或其他方法预先训练好的视频 Embedding 向量，直接作为输入。

         ②在模型训练过程中，通过 Embedding 层动态学习得到的视频向量。（用户向量之后，softmax之前）

     - 相似度计算

       - 在训练阶段，为了将相似度转换为概率，通常会使用 Softmax 函数。

       - 对于每个用户向量 *u* 和视频向量 *v*，计算它们的点积 *u*⋅*v*。点积的结果表示用户向量和视频向量之间的相似度。

         点积越大，说明用户和视频之间的相关性越高。

     - 负采样优化

- **模型训练的经验性知识**

  - 训练数据的选取与生成

    1. 训练数据中对于每个用户选取**相同的样本数**， 保证用户在损失函数等权重， 因为这样可以减少高度活跃用户对于loss的影响。可以改进线上A/B测试的效果。

    2. 在使用滑动窗口生成多个训练样本时，模型**不该知道的信息是未来的用户行为**，**只使用更早时间**的用户行为来产生特征(b)。

       eg.比如一个用户的历史观看记录是"abcdef"， 那么采用滑动窗口， 可以是abc预测d, bcd预测e, cde预测f，这样一个用户就能生成3条训练样本。

       <img src="D:/coding/my_cans/fun-rec/2.BasicAlgorithm/Match/%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B.assets/image-20250219205143994.png" alt="image-20250219205143994" style="zoom:67%;" />



## 总结

| 算法名称 | 算法类别 | 使用场景                                             | 示例                               |
| -------- | -------- | :--------------------------------------------------- | ---------------------------------- |
| UserCF   | 协同过滤 | 用户少， 物品多， 时效性较强的场合                   | 新闻推荐                           |
| ItemCF   | 协同过滤 | 物品少，用户多，用户兴趣固定持久，物品更新速度较慢。 | 推荐艺术品， 音乐， 电影           |
| Swing    | 协同过滤 | 寻找互补商品                                         | 用户完成购买行为以后，推荐互补商品 |
| MF(LFM)  | 协同过滤 |                                                      |                                    |
| FM       | 向量     |                                                      |                                    |
| word2vec | 向量     |                                                      |                                    |
| item2vec | 向量     |                                                      |                                    |
|          |          |                                                      |                                    |
|          |          |                                                      |                                    |





